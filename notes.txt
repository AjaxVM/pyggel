Figure out how to check if we have any data for texcs (or color or whatnot)
Can we check if the vertex attrib is enabled or not?
No we cannot - will need specific shaders for each type of thing, or default all values to something meaningful

Can we check if there is an active texture to use? IE we might get tex coords but have no texture...
Will that just appear white, or how does gl do that.
Also, what does the param for a sampler uniform mean - just passing 0 atm...

get other types of nodes going (render, light)
figure out a good way to let nodes add details without having to have root node need to be able to do all functions
or just bite bullet and do it that way

Will need to make Mesh REQUIRE all fields in it's definition
Can maybe default params if they are missing? IE color: vec4(1), texcs: vec2(0)
Can calculate triangle normals if they are not passed (will need to figure out the normal/parallax maps)

Shaders created to handle Mesh definition X Light definition

Calculate most effictive lights per mesh as:
    Light Intensity / Distance from Mesh
On tie (perhaps with threshold) elect to use closest light
This is simple alternative to deferred lighting perhaps?










Maybe define render passes that have attributes like, whether they affect transparent/not, whether they are lit, etc.
And, they have a shader attached, instead of passing a shader around everywhere



Implement simple HDR with exposure mapping: https://learnopengl.com/Advanced-Lighting/HDR

Render pipeline (forward):
    # use a single shader that can handle normal map/etc. to combine shiz
    Update nodes
    collect render nodes
    split render nodes into opaque/transparent
    order transparent back to front
    order opaque front to back
    collect lights
        Possibly figure out the N most impactful lights to each object (for forward rendering)
    render opaque objects
        bind only the N lights that impact the most + ambient/directional light(s?)
    render transparent objects
        do we need lighting on these or pass for now?

Figure out how to define/store/render materials

Look into using Uniform buffers for lights

Look at subsurface scattering: https://machinesdontcare.wordpress.com/2008/10/29/subsurface-scatter-shader/

Parse model into opaque and transparent bits (using color only, not texture!)
Offset transparent parts to 0 (and remember offset) - subtract by average in each direction
Create a function to get a render node from a mesh - and have it build one and child transform nodes (for offset) with child render node (for transparent part)

TODO: maybe make intensity for lights be calculated in engine and just send color through - so we have fewer calculations to do in gpu?
Maybe we don't care but I imagine those won't change frequently (but then again, consider a fire which will)

Remove the stupid "dirty" stuff - instead allow objects to register callbacks when they are changed

Think about caching node screen positions (others useful at all?) so we can do sorting/culling efficiently
    Inherit position from parent unless we are a TransformNode then calculate our own
    Actually - to do that we need to get LightNode utilizing the transform for position
    Which... makes it so it makes sense to cache our local position
    We should really define somewhere what means what, i.e. generally right now (might be exceptions):
        local position refers to where we want to be in the world - this is represented by the transform matrix
        view position is where in the world our view is position at - this is represented bt the view matrix (camera pos)
        world position is where in the world an object is based on local and offset by view - represented by scene matrix

Create extrapolation function to take indexed set of verts/texcs/etc.
    and map them into 3 verts -> face rather than reusing so normal mapping works better

How to handle reparenting with the root node reference and flat_list with children?

Figure out how the hell to allow meshes with and without texture - or actually make it that you have to pick one or the other.
Or... create a blank white texture to use in that case or something stupid...

DONE: Also, figure out why we can't store the vertex attrib data once... without it getting corrupted
    But, figure out the connection between vertex attribs and vbos and vaos

Consider using a VAO around the vbo and maybe switch away from using PyOpenGL.arrays.vbo
    Definitely do this!

Implement picking in shader as a second output texture (floating point!) with each object having r color indicate object and
    g color indicating face - b and a can be reserved for some other use...

    Could even allow multiple scene picking so that we could have a game scene and a ui scene
        so that stuff behind ui doesn't getted picked if ui is picked

For picking - check the actual alpha of the fragment...
    if it is 0 - then don't render to pick buffer
    otherwise render at alpha 1 (visible or not) to allow some cool non-square ui components...

UI/2D:
    2D scene - handles position adjustments ([0,0] -> [0.5,0.5], [-1,-1] -> [0,0], etc.)
               and absolute positioning calcs (based on screen size)

    text is rendered as glyphs to texture at some resolution (2048 possibly by default, and some size that fits in there?)
        Need to allow maps of other characters to be passed
        Need to store the info needed to position characters from metrics/etc. (possibly find a more compressed way to do this?)
        Rendering glyph positions will convert absolute position to percent based...

    Containers will be nodes on a 2d scene (2d scene being same as regular scene but knowing how to position in window)
    Might want to create vec2/mat3 representations for transforms for ui elements

    How to do container clipping? I don't really want to create an fbo for each container that is rendered to...
    Probably can just calculate min/max X/Y and send to FS for clipping or something like that...

    Allow other types of glyphs to be stored (using custom textures but same offset info as text glyphs)
    Create animation glyph wrapper, that can change colors (gradient or fill), glyph image, position, etc.

Get installation setup.py ready to go so we don't have to have users clone and install reqs themselves

Rotation is messed up when doing more than one axis... for camera at least... wtf?
Also, z rotation for camera appears wrong - everything is warping (with what I assume is the screen width being wider than tall?)

Consider adding a billboard shader as well as supporting the billboard transform node for batched rendering (like particles and such)

remember to run autopep8 with:
    autopep8 -air pyggel
